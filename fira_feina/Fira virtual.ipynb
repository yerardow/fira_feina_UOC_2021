{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRA FEINA VIRTUAL UOC 2021\n",
    "## GERARD COSTA FIGUEROLA - Màster en Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obertura i primera exploració"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Obrim el train dataset\n",
    "df = pd.read_csv('data/uoc_train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2100 entries, 0 to 2099\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   feature1  2100 non-null   float64\n",
      " 1   feature2  2100 non-null   float64\n",
      " 2   feature3  2100 non-null   float64\n",
      " 3   feature4  2100 non-null   float64\n",
      " 4   feature5  2100 non-null   float64\n",
      " 5   feature6  2100 non-null   float64\n",
      " 6   feature7  2100 non-null   float64\n",
      " 7   feature8  2100 non-null   float64\n",
      " 8   target    2100 non-null   int64  \n",
      "dtypes: float64(8), int64(1)\n",
      "memory usage: 147.8 KB\n"
     ]
    }
   ],
   "source": [
    "#Comprovem que no hi hagi valors nuls i els tipus de dades de cada variable\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    713\n",
       "1    705\n",
       "2    682\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mirem quins valors i quants pot tenir la variable original 'target'\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisió del dataset en X i y i en train i test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divideixo en arrays de X i Y el dataframe\n",
    "X = df.values\n",
    "#A la X li treiem la columna 'target' (que és la 8a)\n",
    "X = np.delete(X, 8, axis = 1)\n",
    "\n",
    "#I a la Y li deixem només la columna 'target'\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ara dividirem el dataset en train i test per tal de construir el classificador \n",
    "#Random Forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprovació de les mides dels datasets i com de balancejats estan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mida del train dataset: 1470\n",
      "Mida del test dataset: 630\n",
      "\n",
      "Train dataset\n",
      "{0: 499, 1: 490, 2: 481}\n",
      "\n",
      "Test dataset\n",
      "{0: 214, 1: 215, 2: 201}\n"
     ]
    }
   ],
   "source": [
    "#Imprimim les mides dels dos datasets i mirem com de balancejat està respecte l'original\n",
    "print('Mida del train dataset: {}'.format(len(X_train)))\n",
    "print('Mida del test dataset: {}'.format(len(X_test)))\n",
    "\n",
    "\n",
    "#Per tal que surti un df prou equilibrat, he anat canviant el \n",
    "#random_state al train_test_split anterior\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print('\\nTrain dataset')\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print('\\nTest dataset')\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcció i entrenament del model classificador RF v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ara construim el classificador\n",
    "from sklearn import ensemble\n",
    "\n",
    "rf_clf = ensemble.RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state = 2)\n",
    "\n",
    "#Entrenem al classificador\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generació de prediccions i comprovació de f1 score (macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro): 0.9233299526707235\n"
     ]
    }
   ],
   "source": [
    "#Generem un vector de les prediccions que fa el classificador amb el test dataset\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "f1_v1 = f1_score(y_test, y_pred, average='macro')\n",
    "#I mirem quina f1 score (macro) té\n",
    "print('F1 score (macro): {}'.format(f1_v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent de millorar el model 1 (Hyperparameter tunning)\n",
    "#### Creació de la grid de paràmatres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Està molt bé, però mirem si podem millorar-la fent Hyperparameter tunning amb RandomizedSearchCV\n",
    "\n",
    "#Primer hem de crear la parameter grid per donar-li al gridsearch\n",
    "\n",
    "#Creem diferents vectors amb els diferents valors que volem provar de cada paràmetre\n",
    "n_estimators = np.arange(50, 1200, 100)\n",
    "\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "max_depth = [2, 4, None]\n",
    "\n",
    "min_samples_split = [2, 3, 5]\n",
    "\n",
    "min_samples_leaf = [1,2,3]\n",
    "\n",
    "max_samples = [0.5, 0.75, 1]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "\n",
    "\n",
    "#Creem un diccionari amb tots els paràmetres\n",
    "\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'max_samples': max_samples,\n",
    "              'bootstrap': bootstrap\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenament de la grid per buscar els millors paràmetres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creem el RandomizedSearchCV per veure quins són els millors paràmetres\n",
    "#Aquest, provarà, de totes les combinacions possibles, 100 d'aleatòries i \n",
    "#després podrem saber quina té els millors resultats\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Ho farem mitjançant una funció:\n",
    "\n",
    "#Aquesta funció entrenarà la xarxa provant un nombre determinat de combinacions random\n",
    "#A partir de la xarxa de paràmetres que li passis.\n",
    "#Retornarà un diccionari amb els millors paràmetres\n",
    "\n",
    "def millors_param(classificador, param_grid):\n",
    "    rf_grid = RandomizedSearchCV(estimator = classificador, \n",
    "                             param_distributions = param_grid,\n",
    "                             cv = 10,\n",
    "                             verbose = 0,\n",
    "                             n_jobs = -1)\n",
    "    print('Entrenant grid........ (aprox: 2m30s)')\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    return rf_grid.best_params_\n",
    "    #(Tarda una estona ja que ha de provar unes quantes combinacions random de paràmetres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construcció i entrenament del model amb millors paràmetres segons RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ho farem també amb una funció, que retornarà el nou model, la nova y_pred\n",
    "#i imprimirà i retornarpa la f1 score segons el nou model ambels millors paràmetres \n",
    "#segons RandomizedSearchCV (per veure si millora)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def model_best_param(classificador_original, param_grid, X_train, y_train, X_test, y_test):\n",
    "    best_param = millors_param(classificador_original, param_grid)\n",
    "    print('\\nConstruint model amb els paràmetres:')\n",
    "    print(best_param)\n",
    "    \n",
    "    rf_clf_tunned = ensemble.RandomForestClassifier(n_estimators= best_param['n_estimators'], \n",
    "                                                    n_jobs= -1, \n",
    "                                                    random_state = 2,\n",
    "                                                    min_samples_split= best_param['min_samples_split'],\n",
    "                                                    min_samples_leaf= best_param['min_samples_leaf'], \n",
    "                                                    max_samples= best_param['max_samples'], \n",
    "                                                    max_features=best_param['max_features'], \n",
    "                                                    max_depth= best_param['max_depth'],\n",
    "                                                    bootstrap= best_param['bootstrap'])\n",
    "    rf_clf_tunned.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_tunned = rf_clf_tunned.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred_tunned, average='macro')\n",
    "    print('\\nF1 score (macro): {}'.format(f1))\n",
    "    return [rf_clf_tunned, y_pred_tunned, f1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prova de la funció per buscar el millors paràmetres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenant grid........ (aprox: 2m30s)\n",
      "\n",
      "Construint model amb els paràmetres:\n",
      "{'n_estimators': 750, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_samples': 0.75, 'max_features': 'sqrt', 'max_depth': None, 'bootstrap': False}\n",
      "\n",
      "F1 score (macro): 0.9170614697431314\n"
     ]
    }
   ],
   "source": [
    "#Volem veure doncs si la f1 score millora:\n",
    "\n",
    "rf_clf_v2 = model_best_param(rf_clf, param_grid, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provant-ho diferents vegades amb diferents paràmetres random no aconsegueixo millorar la F1 score incial de 0.92 (en la v2 sempre surt una F1 del voltant de 0.91) \n",
    "\n",
    "És probable doncs que no estigui definint molt bé la parameter grid..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent de millorar el model 2 (Treure variables poc importants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comprovar la importància de cada variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0\n",
      "feature1  0.125156\n",
      "feature2  0.162800\n",
      "feature3  0.230482\n",
      "feature4  0.074054\n",
      "feature5  0.115082\n",
      "feature6  0.234092\n",
      "feature7  0.029971\n",
      "feature8  0.028363\n"
     ]
    }
   ],
   "source": [
    "#Per intentar millorar el model, anem a veure quina és la importància de cada variable\n",
    "#a l'hora de classificar-ho amb el primer model, l'original (que tenia una f1 una mica més alta)\n",
    "\n",
    "cols = df.drop(['target'],1).columns\n",
    "feature_importance_all = rf_clf.feature_importances_\n",
    "feature_importance = {}\n",
    "for i in  range(len(cols)):\n",
    "    feature_importance[cols[i]] = feature_importance_all[i]\n",
    "    \n",
    "feature_import =  pd.DataFrame.from_dict(feature_importance, \n",
    "                                         orient = 'index')\n",
    "print(feature_import)\n",
    "\n",
    "#Veiem que les que més pesen són la feature 3 i la feature6\n",
    "#I les que menys pesen la feature7 i la feature8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tornant a separar en train, test, X i y però treient les dues variables menys importants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La 7 i la 8 no tenen valors molt baixos però per provar si la f1 millora,\n",
    "# les treurem del df train i test i tornarem a provar el model.\n",
    "\n",
    "\n",
    "X2 = df.values\n",
    "#A la X li treiem la columna 'target' (que és la 8a)\n",
    "X2 = np.delete(X2, [6,7,8], axis = 1)\n",
    "\n",
    "#I a la Y li deixem només la columna 'target'\n",
    "y2 = df['target'].values\n",
    "\n",
    "#Tornem a crear els datasets de train i test\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2,y2, test_size=0.3, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tornem a crear, entrenar i mostrar la nova f1 amb el nou classificador v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (macro): 0.9250419331546912\n"
     ]
    }
   ],
   "source": [
    "#Tornem a crear al classificador\n",
    "rf_clf_v3 = ensemble.RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state = 2)\n",
    "\n",
    "#Entrenem al classificador\n",
    "rf_clf_v3.fit(X2_train, y2_train)\n",
    "\n",
    "#El vector de prediccions\n",
    "y_pred2 = rf_clf_v3.predict(X2_test)\n",
    "\n",
    "f1_v3 = f1_score(y2_test, y_pred2, average='macro')\n",
    "\n",
    "#I mirem quina f1 score (macro) té:\n",
    "print('F1 score (macro): {}'.format(f1_v3))\n",
    "\n",
    "#Veiem que té (quasi) la mateixa puntuació, augmenta només en 0.002 punts percentuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumint, tenim 3 models, \n",
    "- El primer sense tocar res \n",
    "- El segon amb els paràmetres modificats \n",
    "- I el tercer treient les dues variables que menys importància tenen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 del primer model: 0.9233299526707235\n",
      "F1 del segon model (tunejat): 0.9170614697431314\n",
      "F1 del tercer model (sense les dues variables menys importants): 0.9250419331546912\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "print('F1 del primer model: {}'.format(f1_v1))\n",
    "print('F1 del segon model (tunejat): {}'.format(rf_clf_v2[2]))\n",
    "print('F1 del tercer model (sense les dues variables menys importants): {}'.format(f1_v3))\n",
    "\n",
    "\n",
    "\n",
    "#Per poc que sigui, ens quedem doncs amb aquest últim model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per tant, ens quedem amb el *3r model*, que té la puntuació f1 més alta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicació del 3r model Random Forest al dataframe uoc_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I ara anem a aplicar aquest útlim model dataframe uoc_X_test\n",
    "import pandas as pd\n",
    "uoc_X_test = pd.read_csv('data/uoc_X_test.csv')\n",
    "\n",
    "#Generem els valors de X, com que utilitzem l'últim model, haurem de treure les dues últimes features.\n",
    "final_X = uoc_X_test.values\n",
    "final_X = np.delete(final_X, [6,7], axis = 1)\n",
    "\n",
    "#Generem els valors predits\n",
    "final_status = rf_clf_v3.predict(final_X)\n",
    "\n",
    "#Els posem en un dataframe\n",
    "final_df = pd.DataFrame(final_status, columns = ['final_status'])\n",
    "\n",
    "#I generem l'arxiu csv final\n",
    "final_df.to_csv('predictions.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
